[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Project 1",
    "section": "",
    "text": "This is my first mini-project for Data Science 2. I looked at coffee prices and how they differed by state.\n\n# Load initial packages required\nlibrary(tidyverse)\nlibrary(mdsr) \n#coffee_data:here(\"https://vinepair.com/articles/price-of-coffee-state-map/\") \n# Create a data frame with coffee prices by state\ncoffee_data &lt;- data.frame(\n  State = c(\"Hawaii\", \"California\", \"Washington\", \"Arizona\", \"Massachusetts\",\n            \"Colorado\", \"Utah\", \"Florida\", \"Vermont\", \"Connecticut\",\n            \"New Jersey\", \"New Mexico\", \"Alaska\", \"Louisiana\", \"Maryland\",\n            \"New York\", \"Nevada\", \"Oregon\", \"South Carolina\", \"New Hampshire\",\n            \"Rhode Island\", \"Maine\", \"Virginia\", \"Illinois\", \"Kentucky\",\n            \"Georgia\", \"Texas\", \"Delaware\", \"Idaho\", \"North Carolina\",\n            \"Pennsylvania\", \"Tennessee\", \"Oklahoma\", \"Minnesota\", \"Wyoming\",\n            \"Missouri\", \"Alabama\", \"Arkansas\", \"Michigan\", \"Ohio\",\n            \"Indiana\", \"Mississippi\", \"Wisconsin\", \"Iowa\", \"South Dakota\",\n            \"West Virginia\", \"North Dakota\", \"Kansas\", \"Montana\", \"Nebraska\"),\n  \n  Price_of_Coffee = c(4.98, 3.88, 3.69, 3.51, 3.49, 3.43, 3.40, 3.38,\n                      3.32, 3.31, 3.31, 3.31, 3.27, 3.26, 3.25, 3.24,\n                      3.23, 3.21, 3.17, 3.13, 3.13, 3.08, 3.08, 3.07,\n                      3.03, 3.02, 2.99, 2.97, 2.94, 2.94, 2.94, 2.91,\n                      2.90, 2.89, 2.89, 2.88, 2.86, 2.86, 2.83, 2.79,\n                      2.78, 2.77, 2.76, 2.70, 2.66, 2.65, 2.64, 2.59,\n                      2.56, 2.12)\n)\n\n# Load US states map data\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n# Calculate statistics for coffee prices\nmean_price &lt;- mean(coffee_data$Price_of_Coffee)\nmax_price &lt;- max(coffee_data$Price_of_Coffee)\nmin_price &lt;- min(coffee_data$Price_of_Coffee)\n\n# Separate coffee data into above and below mean\nabove_mean &lt;- coffee_data[coffee_data$Price_of_Coffee &gt; mean_price, ]\nbelow_mean &lt;- coffee_data[coffee_data$Price_of_Coffee &lt; mean_price, ]\n\n# Categorize coffee prices as Above or Below Mean\ncoffee_data &lt;- coffee_data %&gt;%\n  mutate(Price_Group = ifelse(Price_of_Coffee &gt; mean_price, \"Above Mean\", \"Below Mean\"))\n\n# Merge coffee data with state map data\nmerged_data &lt;- us_states %&gt;%\n  left_join(coffee_data, by = c(\"region\" = \"State\"))\n\n# Basic US states map\nus_states |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n  geom_polygon(fill = \"white\", color = \"black\") +\n  theme_minimal() +\n  labs(title = \"US States Map\")\n\n\n\n\n\n\n\n# Prepare coffee data for plotting\ncoffee_data &lt;- coffee_data |&gt;\n  mutate(State = str_to_lower(State))\n\n# Create a map of coffee prices by state\ncoffee_data |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n  geom_polygon(aes(fill = Price_of_Coffee), color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Coffee Prices by State\", fill = \"Price of Coffee ($)\")\n\n\n\n\n\n\n\n# Merge coffee data with state map data\nmerged_data0 &lt;- us_states %&gt;%\n  left_join(coffee_data, by = c(\"region\" = \"State\"))\n\nA map of the United States displaying coffee prices by state. Each state is filled with a gradient color representing the price of coffee, with darker colors indicating higher prices. A color legend on the side shows the corresponding price ranges. We can see that Nebraska is the darkest, meaning it has the lowest coffee price. California is the lightest along with Washinton, indicating that they have higher coffee prices.\n\nFinal map showing coffee price groups\n\nggplot(data = merged_data0, aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = Price_Group), color = \"darkgrey\", linewidth = 0.2) +\n  labs(fill = \"Coffee Price Group\") +\n  coord_fixed() + \n  theme_void() +  \n  scale_fill_manual(values = c(\"brown1\", \"brown4\")) +\n  ggtitle(\"Coffee Prices Across the USA\")\n\n\n\n\n\n\n\n\nA map of the United States illustrating coffee price groups. States are colored in varying shades of brown to indicate whether their coffee prices are above or below the mean price. A dark grey outline defines each state, and a legend indicates the coffee price categories. We can see that “Below Mean” is more in the middle of the US. The “Above Mean” is on the west coast and on the upper east coast.\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(leaflet)\n# Load spatial data for US states\nlibrary(sf)  # Load sf for spatial data\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")  \nclass(states)  # Check the class of the states object\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# Merge coffee data with state map data using spatial data\nmerged_data2 &lt;- states %&gt;%\n  mutate(name = str_to_lower(name)) %&gt;%\n  left_join(coffee_data, by = c(\"name\" = \"State\"))\n\n# Create bins for coffee prices\nbins &lt;- c(0, 2.5, 3.0, 3.5, 5.0, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = merged_data$Price_of_Coffee, bins = bins)\n\n# Create labels for hovering\nmerged_data2 &lt;- merged_data2 %&gt;%\n  mutate(label = glue(\"{name}: ${Price_of_Coffee}\"))\n\n# Create the leaflet map\nleaflet(data = merged_data2) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(Price_of_Coffee),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE\n    ),\n    label = ~label,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal,\n    values = ~Price_of_Coffee,\n    opacity = 0.7,\n    title = \"Coffee Price ($)\",\n    position = \"bottomright\"\n  )\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data\n\n\n\n\n\n\nAn interactive map of the United States displaying coffee prices by state. Each state is filled with a color gradient representing coffee prices, with hover labels showing the state name and specific coffee price. A color legend in the bottom right corner indicates the price ranges, enhancing the visualization of coffee prices across the country. We can see the exact price for each state."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/)."
  },
  {
    "objectID": "project2.html#penguin-random-house-web-scraping",
    "href": "project2.html#penguin-random-house-web-scraping",
    "title": "Mini Project 2",
    "section": "",
    "text": "Our data comes from the events page in Penguin Random House. You can find the events page using [the Penguin Random House events page]: (https://www.penguinrandomhouse.com/authors/events/)."
  },
  {
    "objectID": "project2.html#ethical-considerations",
    "href": "project2.html#ethical-considerations",
    "title": "Mini Project 2",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nWe opted to use the robot.txt paths to determine if data from Penguin’s book events was permitted for web scraping. The site’s robots.txt file allowed our bots to access and scrape the data. While we considered implementing a polite function to ensure a respectful approach to data retrieval, this step appeared unnecessary given that the data is public and intended for widespread use."
  },
  {
    "objectID": "project2.html#novel-insights-potential-and-justification",
    "href": "project2.html#novel-insights-potential-and-justification",
    "title": "Mini Project 2",
    "section": "Novel Insights Potential and Justification",
    "text": "Novel Insights Potential and Justification\nOur final tibble will hold important information for booksellers, authors, agents, and students to utilize in regards to books/authors from Penguin Random House. We were initially motivated to explore book events from Penguin Random House to inform student decisions to network with agents and authors at various events.\nStudents can use our data to answer questions such as:\n\n“Where are events most commonly held?”\n“Which season has the most book events?”\n“What are the best events to attend to network with the right authors and book genres?”\n\nUpon further reflection, we discovered that our data could also be used by booksellers, book agents, and authors. Booksellers and authors may find our data useful because they can analyze current trends with where authors are going (chain or independent bookstore) and what authors are successful in book events (if we assume multiple book events equals a marketable author).Book agents within Penguin Random House or outside of it (smaller boutique literary agencies or other Big Five publishers) can use our data to answer questions on which authors are holding events, when a certain book is no longer welcomed in event spaces, and perhaps even publicity tactics. Ultimately, this data has relevant applications for different data needs within the publishing industry, not just students.\n\n#Step 0: Check if the website allows scraping \nrobotstxt::paths_allowed(\"https://www.penguinrandomhouse.com/authors/events/\")\n\n\n www.penguinrandomhouse.com                      \n\n\n[1] TRUE\n\n#Extract individual information from the events page \ninfo_from_page &lt;- function(event, css_selector) {\n  read_html(event) |&gt; \n#Extracting nodes from the XML by using the CSS path from selector\n  html_nodes(css_selector) |&gt; \n#Extracting text\n  html_text()\n}\n\n#Test, the function works\ninfo_from_page(\"https://www.penguinrandomhouse.com/authors/events/\", \".date-display\")\n\n[1] \"December 2024\"\n\n\n\n#Scrape info using the CSS path and compile it into a tibble \nscrape_events &lt;- function(url){\n  \n  date &lt;- info_from_page(url, \".start\")\n  book &lt;- info_from_page(url, \".author-of a\")\n  author &lt;- info_from_page(url, \".author-name:nth-child(1)\")\n  host &lt;- info_from_page(url, \".event-location .hdr\")\n  state &lt;- info_from_page(url, \"span:nth-child(4)\")\n  zip_code &lt;- info_from_page(url, \"span:nth-child(5)\")\n  \n  tibble(date = date, \n           book = book, \n           author = author,\n           host = host,\n           state = state)\n  \n}\n\n#Test to see that our tibble looks appropriate\nscrape_events(\"https://www.penguinrandomhouse.com/authors/events/?page=2\")\n\n# A tibble: 9 × 5\n  date                   book                           author       host  state\n  &lt;chr&gt;                  &lt;chr&gt;                          &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;\n1 12/14/2024 at 12pm     Up Island Harbor               Jean Stone   EDGA… MA   \n2 12/14/2024 at 2:00pm   Sir Callie and the Witch’s War Esme Symes-… BARN… MO   \n3 12/14/2024 at 5 PM     Alice in a Winter Wonderland   Jan Brett    Bell… WA   \n4 12/14/2024 at 10:00 am The Little Chefs               Rosemary We… BARR… CT   \n5 12/14/2024 at 10 AM    Safe Harbor                    Padma Venka… New … MA   \n6 12/14/2024             I Am the Grinch                Alastair He… Join… VA   \n7 12/14/2024             I Am the Grinch                Alastair He… Join… PA   \n8 12/14/2024             I Am the Grinch                Tom Brannon  Join… CT   \n9 12/14/2024 at 11:00am  Christmas Forever              Elysa Dutton CHEV… CA   \n\n\n\n#This for loop runs all of the months and all of the days\n#   in one chunk but it is not the most efficient \n\n#If someone is interested in keeping this method in one chunk\n#   they can use this code for the nested for loop.\n\n#Nested for loop with i for months and j for days. \nfor(i in c(10, 11, 12, 1, 2, 3, 4)){\n#Runs to find data for all of the dates in these months\n#   we can compile all of the data together \n  for(j in 1:31){\n#Combining i and j for the dates to keep track of event dates\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    scrape_events(url)\n  }\n}\n\n\n#Running each individual month as a separate \n#   for loop to be more efficient\n\n#Create a list to store your scraped data\noctober &lt;- list()\n  i=10\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    october[[j]] &lt;- scrape_events(url)\n}\n\n#Create a tibble from the list \noctober_tibble &lt;- bind_rows(october) |&gt; \n  as_tibble()\n  \nnovember &lt;- list()\n  i=11\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    november[[j]] &lt;- scrape_events(url)\n}\n  \n  november_tibble &lt;- bind_rows(november) |&gt; \n    as_tibble() \n  \ndecember &lt;- list()\n  i=12\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2024\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    december[[j]] &lt;- scrape_events(url)\n}\n  \n  december_tibble &lt;- bind_rows(december) |&gt; \n    as_tibble()\n  \njanuary &lt;- list()\n  i=1\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    january[[j]] &lt;- scrape_events(url)\n}\n  \njanuary_tibble &lt;- bind_rows(january) |&gt; \n  as_tibble()\n\nfebruary &lt;- list()\n  i=2\nfor(j in 1:28){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    february[[j]] &lt;- scrape_events(url)\n}\n  \n  february_tibble &lt;- bind_rows(february) |&gt; \n    as_tibble() \n  \nmarch &lt;- list()\n  i=3\nfor(j in 1:31){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    march[[j]] &lt;- scrape_events(url)\n}\n  \n  march_tibble &lt;- bind_rows(march) |&gt; \n    as_tibble() \n  \napril &lt;- list()\n  i=4\nfor(j in 1:30){\n    date = str_c(i, \"/\", j, \"/\", \"2025\")\n    url = str_c(\n      \"https://www.penguinrandomhouse.com/authors/events/?datefrom=\",\n      date, \n      \"&dateto=\", \n      date)\n    april[[j]] &lt;- scrape_events(url)\n}\n  \napril_tibble &lt;- bind_rows(april) |&gt; \n  as_tibble()\n\n\n#Bind all of the tibbles from the previous chunk \n#   together to create one big tibble called events \n\nevents &lt;- rbind(october_tibble,\n                 november_tibble,\n                 december_tibble,\n                 january_tibble,\n                 february_tibble,\n                 march_tibble,\n                 april_tibble)\nevents\n\n\npenguin_events &lt;- events |&gt; \n#Separate the time from date to create a separate column\n#   for the time of the events\n  separate(date, into = c(\"date\", \"time\"), sep = \" at \") |&gt; \n#Some of the host names were all caps or had other \n#   abnormalities that we needed to fix\n  mutate(host = str_to_title(host),\n#Some observations had strange patterns in the text (r/n) \n#   that distracted from the host's name \n         host = str_replace_all(host, \"[\\r\\n]\", \" \"))\npenguin_events"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "George James\nhe/him/his\nStudent at St. Olaf College"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nStudent at St. Olaf College\n\nFocus on academic studies, active engagement in college community, and personal development."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\n\n📧 🐙"
  }
]